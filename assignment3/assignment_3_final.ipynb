{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.axes import Axes\n",
    "from typing import Tuple\n",
    "from tqdm import tqdm\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Modelling exercise 3: Temporal learning and acting\n",
    "\n",
    "## TD Learning\n",
    "\n",
    "Recreate figure 9.2 (page 15) of the book chapter we provided.\n",
    "\n",
    "Experiment with following parameters. Plot and briefly describe your observations for each.\n",
    "\n",
    "- Reward timing\n",
    "- Learning rate\n",
    "- Multiple rewards\n",
    "- Stochastic rewards\n",
    "\n",
    "(Note that you will have to think about how to represent the time between the stimulus and the reward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def add_stimulus(series: np.ndarray, t: int = 100, std: float = 0):\n",
    "    # add a gaussian stimulus at time t and with distribution std\n",
    "    if std != 0:\n",
    "        raise NotImplementedError(\"Currently assuming only point mass stimulus\")\n",
    "\n",
    "    series[t] += 1\n",
    "    return series\n",
    "\n",
    "\n",
    "def add_reward(series: np.ndarray, t: int = 200, std: float = 2):\n",
    "    x = np.arange(len(series))\n",
    "    y = 1 / (std * np.sqrt(2 * np.pi)) * np.exp(-0.5 * (x - t) ** 2 / std ** 2)\n",
    "    return series + y\n",
    "\n",
    "\n",
    "length = 300\n",
    "stimulus = np.zeros(length)\n",
    "stimulus = add_stimulus(stimulus)\n",
    "reward = np.zeros(length)\n",
    "reward = add_reward(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def get_value(weights: np.ndarray, stimulus: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates the total future reward using the current weights.\n",
    "    \n",
    "    :param weights: vector of weights\n",
    "    :param stimulus: vector of stimuli\n",
    "    :return: total future reward\n",
    "    \"\"\"\n",
    "    v = np.zeros_like(weights)\n",
    "    length = len(weights)\n",
    "    for t in range(length):\n",
    "        v[t] = np.dot(weights[:t], stimulus[1: t + 1][::-1])\n",
    "    return v\n",
    "\n",
    "\n",
    "def get_delta(reward: np.ndarray, value: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates the difference between the actual and predicted total future reward.\n",
    "    \n",
    "    :param reward: actual future reward\n",
    "    :param value: predicted total future reward\n",
    "    :return: difference between the actual and predicted total future reward\n",
    "    \"\"\"\n",
    "    future_value = value[1:]\n",
    "    current_value = value[:-1]\n",
    "    delta = reward[:-1] + future_value - current_value\n",
    "    return delta\n",
    "\n",
    "\n",
    "def learn_step(weights: np.ndarray, stimulus: np.ndarray, delta: np.ndarray, epsilon: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates the weights for a single learning step.\n",
    "    \n",
    "    :param weights: current weights\n",
    "    :param stimulus: vector of stimuli\n",
    "    :param delta: difference between the actual and predicted total future reward\n",
    "    :param epsilon: learning rate\n",
    "    :return: updated vector of weights\n",
    "    \"\"\"\n",
    "    length = len(weights)\n",
    "    for t in range(length - 1):\n",
    "        weights[:t] += epsilon * delta[t] * stimulus[1: t + 1][::-1]\n",
    "    return weights\n",
    "\n",
    "\n",
    "def td_learning(stimulus: np.ndarray, reward: np.ndarray, epsilon: float = 0.98, n_trials: int = 100):\n",
    "    \"\"\"\n",
    "    Temporal difference learning.\n",
    "    \n",
    "    :param stimulus: vector of stimuli\n",
    "    :param reward: vector of rewards\n",
    "    :param epsilon: learning rate\n",
    "    :param n_trials: number of trials\n",
    "    :return: history of temporal difference learning, containing value, weight,\n",
    "             and delta change over time\n",
    "    \"\"\"\n",
    "    if len(stimulus.shape) == 1:\n",
    "        # stack it n_trials open \n",
    "        stimulus = stimulus[None].repeat(n_trials, axis = 0)\n",
    "    elif len(stimulus.shape) == 2:\n",
    "        assert len(stimulus) == n_trials\n",
    "    else:\n",
    "        raise ValueError(\"stimulus does not have the right shape\")\n",
    "\n",
    "    if len(reward.shape) == 1:\n",
    "        # stack it n_trials open \n",
    "        reward = reward[None].repeat(n_trials, axis = 0)\n",
    "    elif len(reward.shape) == 2:\n",
    "        assert len(reward) == n_trials\n",
    "    else:\n",
    "        raise ValueError(\"reward does not have the right shape\")\n",
    "\n",
    "    weights = np.zeros(stimulus.shape[1])\n",
    "    history = {\n",
    "        \"value\": [],\n",
    "        \"weights\": [],\n",
    "        \"deltas\": [],\n",
    "    }\n",
    "    for epoch_idx in tqdm(range(n_trials)):\n",
    "        value = get_value(weights, stimulus[epoch_idx])\n",
    "        deltas = get_delta(reward[epoch_idx], value)\n",
    "        weights = learn_step(weights, stimulus[epoch_idx], deltas, epsilon)\n",
    "\n",
    "        history[\"deltas\"].append(deltas)\n",
    "        history[\"value\"].append(value)\n",
    "        history[\"weights\"].append(weights)\n",
    "\n",
    "    for key in history.keys():\n",
    "        history[key] = np.stack(history[key])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_surface(z: np.ndarray, label: str = \"\"):\n",
    "    fig, ax = plt.subplots(subplot_kw = { \"projection\": \"3d\" })\n",
    "\n",
    "    # Make data.\n",
    "    X = np.arange(z.shape[1])\n",
    "    Y = np.arange(z.shape[0])\n",
    "    X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "    # Plot the surface.\n",
    "    ax.view_init(15, -77, 0)\n",
    "    ax.plot_surface(X, Y, z, cmap = cm.seismic)\n",
    "    ax.set_xlabel(\"t\")\n",
    "    ax.set_ylabel(\"trials\")\n",
    "    ax.set_zlabel(label)\n",
    "\n",
    "\n",
    "def plot_comparison(stimulus, reward, history):\n",
    "    fig, axes = plt.subplots(nrows = 5, ncols = 2, sharex = True, sharey = \"row\")\n",
    "    axes[0, 0].set_title(\"before\")\n",
    "    axes[0, 1].set_title(\"after\")\n",
    "    axes[0, 1].set_ylabel(r\"$u$\")\n",
    "    axes[1, 1].set_ylabel(r\"$r$\")\n",
    "    axes[2, 1].set_ylabel(r\"$v$\")\n",
    "    axes[3, 1].set_ylabel(r\"$\\triangle v$\")\n",
    "    axes[4, 1].set_ylabel(r\"$\\delta$\")\n",
    "\n",
    "    axes[0, 0].plot(stimulus, c = \"k\")\n",
    "    axes[0, 1].plot(stimulus, c = \"k\")\n",
    "\n",
    "    axes[1, 0].plot(reward, c = \"k\")\n",
    "    axes[1, 1].plot(reward, c = \"k\")\n",
    "\n",
    "    axes[2, 0].plot(history[\"value\"][0], c = \"k\")\n",
    "    axes[2, 1].plot(history[\"value\"][-1], c = \"k\")\n",
    "\n",
    "    axes[3, 0].plot(np.diff(history[\"value\"][0]), c = \"k\")\n",
    "    axes[3, 1].plot(np.diff(history[\"value\"][-1]), c = \"k\")\n",
    "\n",
    "    axes[4, 0].plot(history[\"deltas\"][0], c = \"k\")\n",
    "    axes[4, 1].plot(history[\"deltas\"][-1], c = \"k\")\n",
    "\n",
    "    axes[4, 0].set_xlabel(\"t\")\n",
    "    axes[4, 1].set_xlabel(\"t\")\n",
    "\n",
    "    for ax in axes.flatten():\n",
    "        ax.axvline(100, color = \"k\", linestyle = \"dotted\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Recreating the figure 9.2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epsilon = 0.8\n",
    "n_trials = 200\n",
    "history = td_learning(stimulus, reward, n_trials = n_trials, epsilon = epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "plot_surface(history[\"deltas\"], label = r\"$\\delta$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We ran the temporal difference learning algorithm for 200 trials with a learning rate of 0.8, presenting the stimulus at time $t = 100$ and the reward at time $t = 200$. As expected, we notice that the prediction error over time shifts towards the time of the stimulus."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "plot_comparison(stimulus, reward, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before the trials, no prediction $v$ for the future reward is made (and thus, no difference between the predictions $\\Delta v$ is visible). The difference between the actual and predicted total future reward $\\delta$ is equal to the actual reward.\n",
    "\n",
    "After the trials, the predicted total future reward $v$ suddenly increases at the time of the stimulus, and then drops once the reward is presented. The temporal difference between predictions $\\Delta v$ resembles a delta function at the time of the stimulus and dips during the time point of the reward (as expected, the largest difference between the predictions $v$ is during the stimulus onset; the dip can be explained by the falling phase of the predictions $v$). The difference between the actual and predicted total future reward $\\delta$ shifts in time to the stimulus onset, and predicts full reward at that time point."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "stimulus = np.zeros(length)\n",
    "stimulus[100] = 1\n",
    "reward = np.zeros(length)\n",
    "reward = add_reward(reward, 150)\n",
    "\n",
    "history = td_learning(stimulus, reward, epsilon, n_trials)\n",
    "plot_surface(history[\"deltas\"], label = r\"$\\delta$\")\n",
    "plot_comparison(stimulus, reward, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We presented the reward at time $t = 150$, maintaining the stimulus time at $t = 100$. The results resemble the case when the reward is presented at time $t = 200$, with the difference between the actual and predicted total future reward $\\delta$ shifting in time to the time point of the stimulus onset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "stimulus = np.zeros(length)\n",
    "stimulus[100] = 1\n",
    "reward = np.zeros(length)\n",
    "reward = add_reward(reward, 50)\n",
    "\n",
    "history = td_learning(stimulus, reward, epsilon, n_trials)\n",
    "plot_surface(history[\"deltas\"], label = r\"$\\delta$\")\n",
    "plot_comparison(stimulus, reward, history)\n",
    "\n",
    "#TODO!!"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we presented the reward at time $t = 50$, maintaining the stimulus time at $t = 100$. In this case, the difference between the actual and predicted total future reward $\\delta$ does not shift to the time of the stimulus."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "stimulus = np.zeros(length)\n",
    "stimulus[100] = 1\n",
    "reward = np.zeros(length)\n",
    "reward = add_reward(reward, 200)\n",
    "\n",
    "history = td_learning(stimulus, reward, 0.50, n_trials)\n",
    "plot_surface(history[\"deltas\"], label = r\"$\\delta$\")\n",
    "plot_comparison(stimulus, reward, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the case of a lower learning rate, the predictions $v$, difference between predictions $\\delta v$ and the difference between the actual and predicted total future reward $\\delta$ do not fully converge in the same number of trials. The weights are updated in a slower manner, and thus it takes longer to fully form the association between the stimulus and the reward."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "stimulus = np.zeros(length)\n",
    "stimulus[100] = 1\n",
    "reward = np.zeros(length)\n",
    "reward = add_reward(reward, 150)\n",
    "reward = add_reward(reward, 200)\n",
    "\n",
    "history = td_learning(stimulus, reward, epsilon, n_trials)\n",
    "plot_surface(history[\"deltas\"], label = r\"$\\delta$\")\n",
    "plot_comparison(stimulus, reward, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we present a stimulus at time $t = 100$ and two rewards: one at time $t = 150$, and another at time $t = 200$. The predicted future reward $v$ drops after the first reward is presented to half of its maximal value, and eventually to zero after the presentation of both rewards. The difference between predictions $\\delta v$ in this case has two dips instead of one, relating to the two drops in the total predicted future reward $v$. The difference between the actual and predicted total future reward $\\delta$ is shifted in time to the time point of the stimulus to predict both rewards."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# TODO: what is going on here? How were the rewards generated? + they are not presented in the plot\n",
    "# TODO: is random reward = random timing, or random quantity of the reward?\n",
    "# TODO: I don't think this is right\n",
    "\n",
    "stimulus = np.zeros(length)\n",
    "stimulus[100] = 1\n",
    "reward = np.zeros(length)\n",
    "reward = add_reward(reward, 200)\n",
    "binary_mask = np.random.binomial(1, 0.9, n_trials)\n",
    "random_reward = reward[None] * binary_mask[:, None]\n",
    "\n",
    "history = td_learning(stimulus, random_reward, epsilon, n_trials)\n",
    "plot_surface(history[\"deltas\"], label = r\"$\\delta$\")\n",
    "plot_comparison(stimulus, reward, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Successor learning\n",
    "\n",
    "\n",
    "We will learn the successor representation (discounted future state occupancies) for a simple grid-world under a random walk policy. We will use the grid-world of the shown figure, refer to the Python code we provided for an implementation of this grid.\n",
    "\n",
    "Implement a random walk policy (up, down, left, or right with equal probability), filling in the provided function stub. Given a maze, a starting location, and a number of steps, perform the specified number of random moves from the starting location. Make sure to exclude impossible moves (don’t just stay at the current spot when such a move is attempted, but pick a different one instead). Use the provided function to plot such a trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# define maze\n",
    "\n",
    "maze = np.zeros((9, 13))\n",
    "\n",
    "# place walls\n",
    "maze[2, 6:10] = 1\n",
    "maze[-3, 6:10] = 1\n",
    "maze[2:-3, 6] = 1\n",
    "\n",
    "# define start\n",
    "start = (6, 8)\n",
    "\n",
    "# pad maze\n",
    "pad = np.ones(np.array(maze.shape) + 2)\n",
    "pad[1:-1, 1:-1] = maze\n",
    "maze = pad\n",
    "\n",
    "\n",
    "def plot_maze(maze) -> Tuple[Figure, Axes]:\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(maze, cmap = 'binary')\n",
    "\n",
    "    # draw thin grid\n",
    "    for i in range(maze.shape[0]):\n",
    "        ax.plot([-0.5, maze.shape[1] - 0.5], [i - 0.5, i - 0.5], c = 'gray', lw = 0.5)\n",
    "    for i in range(maze.shape[1]):\n",
    "        ax.plot([i - 0.5, i - 0.5], [-0.5, maze.shape[0] - 0.5], c = 'gray', lw = 0.5)\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "plot_maze(maze)\n",
    "plt.scatter(start[1], start[0], marker = '*', color = 'blue', s = 100)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def random_walk(maze, start, n_steps):\n",
    "    # Perform a single random walk in the given maze, starting from start, performing n_steps random moves.\n",
    "    # Moves into the wall and out of the maze boundary are not possible.\n",
    "\n",
    "    # initialize list to store positions\n",
    "    positions = np.empty((n_steps + 1, len(maze.shape)), dtype = int)\n",
    "    positions[0] = start\n",
    "\n",
    "    # perform random steps...\n",
    "    possible_moves = np.array([[-1, 0], [1, 0], [0, -1], [0, 1]], dtype = int)\n",
    "    for i in range(n_steps):\n",
    "        move_options = positions[i][None] + possible_moves\n",
    "        # remove those moves which run into a barrier\n",
    "        move_options = 1 - maze[*move_options.T]\n",
    "        # int prob distribution\n",
    "        p = move_options / move_options.sum()\n",
    "        move_idx = np.random.choice(len(possible_moves), p = p)\n",
    "        move = possible_moves[move_idx]\n",
    "        positions[i + 1] = positions[i] + move\n",
    "\n",
    "    # return a list of length n_steps + 1, containing the starting position and all subsequent locations as e.g. tuples or size (2) arrays \n",
    "    return positions\n",
    "\n",
    "\n",
    "def plot_path(maze, path):\n",
    "    # plot a maze and a path in it\n",
    "    plot_maze(maze)\n",
    "    path = np.array(path)\n",
    "    plt.plot(path[:, 1], path[:, 0], c = 'red', lw = 3)\n",
    "    plt.scatter(path[0, 1], path[0, 0], marker = '*', color = 'blue', s = 100)\n",
    "    plt.scatter(path[-1, 1], path[-1, 0], marker = '*', color = 'green', s = 100)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# plot a random path\n",
    "path = random_walk(maze, start, 40)\n",
    "plot_path(maze, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Write a function which takes a trajectory of grid positions and the current state of your learned successor representation (SR; for this environment it is practical to implement the SR as a matrix, corresponding to the grid). Then, based on the provided trajectory, update the successor representation matrix of the starting state, being sure to discount future states appropriately. Repeat this a number of times with different examples, as outlined in our code, to learn a representation from examples. Plot the learned representation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def learn_from_traj(succ_repr, trajectory, gamma = 0.98, alpha = 0.02):\n",
    "    # Write a function to update a given successor representation (for the state at which the trajectory starts) using an example trajectory\n",
    "    # using discount factor gamma and learning rate alpha\n",
    "\n",
    "    # TODO: where is alpha?\n",
    "    discount = 1\n",
    "    for point in trajectory:\n",
    "        succ_repr[*point] += discount\n",
    "        discount *= gamma\n",
    "\n",
    "    # return the updated successor representation\n",
    "    return succ_repr\n",
    "\n",
    "\n",
    "# initialize successor representation\n",
    "succ_repr = np.zeros_like(maze)\n",
    "\n",
    "# sample a whole bunch of trajectories (reduce this number if this code takes too long, but it shouldn't take longer than a minute with reasonable code)\n",
    "for i in tqdm(range(5001)):\n",
    "    # sample a path (we use 340 steps here to sample states until the discounting becomes very small)\n",
    "    path = random_walk(maze, start, 340)\n",
    "    # update the successor representation\n",
    "    succ_repr = learn_from_traj(succ_repr, path, alpha = 0.02)  # choose a small learning rate\n",
    "\n",
    "    # occasionally plot it\n",
    "    if i in [0, 10, 100, 1000, 5000]:\n",
    "        fig, ax = plot_maze(maze)\n",
    "        ax.set_title(f\"Successor Representation after {i + 1} epochs\")\n",
    "        ax.imshow(succ_repr, cmap = 'hot')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compute the overall transition matrix, based on the maze layout. Note that this matrix has size 117×117 (as 9*13=117), to represent the probability for transitioning from any state into any other. (Rows which correspond to an impossible state, a wall, contain only zeros, make sure to remove NaNs here)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def compute_transition_matrix(maze: np.ndarray) -> np.ndarray:\n",
    "    # for a given maze, compute the transition matrix from any state to any other state under a random walk policy\n",
    "    # (you will need to think of a good way to map any 2D grid coordinates onto a single number for this)\n",
    "\n",
    "    # create a matrix over all state pairs\n",
    "    n_states = len(maze.flatten())\n",
    "    transitions = np.zeros((n_states, n_states))\n",
    "\n",
    "    indices = np.arange(n_states).reshape(maze.shape)\n",
    "\n",
    "    # only vertical or horizontal moves allowed\n",
    "    possible_moves = np.array([[-1, 0], [1, 0], [0, -1], [0, 1]], dtype = int)\n",
    "\n",
    "    # allow diagonal moves\n",
    "    # possible_moves = np.array([[-1, 0], [1, 0], [0, -1], [0, 1], [1, 1], [1, -1], [-1, 1], [-1, -1]], dtype=int)\n",
    "\n",
    "    # iterate over all states, filling in the transition probabilities to all other states on the next step (only one step into the future)\n",
    "    for state_idx, (i, j) in enumerate(\n",
    "            product(range(maze.shape[0]), range(maze.shape[1]))\n",
    "    ):\n",
    "        cell = maze[i, j]\n",
    "        if cell == 1:\n",
    "            continue\n",
    "\n",
    "        coords = np.array([i, j])\n",
    "        move_options = coords + possible_moves\n",
    "        future_cell_values = maze[*move_options.T]\n",
    "        move_options = move_options[~future_cell_values.astype(bool)]\n",
    "        future_states = indices[*move_options.T]\n",
    "        # print(coords, future_states)\n",
    "        transitions[state_idx, future_states] += 1\n",
    "        # normalize transitions if necessary\n",
    "        transitions[state_idx] /= transitions[state_idx].sum()\n",
    "\n",
    "    return transitions\n",
    "\n",
    "\n",
    "transitions = compute_transition_matrix(maze)\n",
    "n_states = len(transitions)\n",
    "\n",
    "one_hot_start = np.zeros(n_states)\n",
    "one_hot_start[start[0] * maze.shape[1] + start[1]] = 1\n",
    "\n",
    "indices = np.arange(n_states).reshape(maze.shape)\n",
    "\n",
    "succ_repr = np.zeros(n_states)\n",
    "tx = np.eye(n_states)\n",
    "gamma = 0.98\n",
    "for i in range(340):\n",
    "    succ_repr += tx.T @ one_hot_start\n",
    "    tx = gamma * tx @ transitions\n",
    "\n",
    "_, ax = plot_maze(maze)\n",
    "ax.imshow(succ_repr.reshape(maze.shape), cmap = \"hot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Recompute the successor representation at the starting position by repeatedly applying the transition matrix you computed (for this it is opportune to turn the SR matrix into a vector)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def compute_sr(transitions, i, j, maze, gamma = 0.98):\n",
    "    \"\"\"\n",
    "    given a transition matrix and a specific state (i, j).\n",
    "    compute the successor representation of that state with discount factor gamma\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize things (better to represent the current discounted occupancy as a vector here)\n",
    "    n_states = len(transitions)\n",
    "    current_discounted_occupancy = np.zeros(n_states)\n",
    "    total = current_discounted_occupancy.copy()\n",
    "\n",
    "    one_hot_start = np.zeros(n_states)\n",
    "    one_hot_start[i * maze.shape[1] + j] = 1\n",
    "\n",
    "    # iterate for a number of steps\n",
    "    tx = np.eye(n_states)\n",
    "    for _ in range(340):\n",
    "        current_discounted_occupancy = tx.T @ one_hot_start\n",
    "        total += current_discounted_occupancy\n",
    "        tx = gamma * tx @ transitions\n",
    "\n",
    "    # return the successor representation, maybe reshape your vector into the maze shape now\n",
    "    return total.reshape(maze.shape)\n",
    "\n",
    "\n",
    "sr = compute_sr(transitions, *start, maze)\n",
    "_, ax = plot_maze(maze)\n",
    "ax.imshow(sr, cmap = \"hot\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
