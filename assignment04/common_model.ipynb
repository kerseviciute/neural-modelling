{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-06T23:51:27.720300Z",
     "start_time": "2024-12-06T23:51:27.024580Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Model(ABC):\n",
    "    def __init__(self):\n",
    "        # Init model fit bounds\n",
    "        self.epsilon_bounds = (0.0000001, 0.99999)\n",
    "        self.beta_bounds = (0.0001, 9.9999)\n",
    "        self.bias_bounds = (-0.99999, 0.99999)\n",
    "        self.p_bounds = (0.0001, 0.9999)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        exp = np.exp(x)\n",
    "        return exp / np.sum(exp)\n",
    "\n",
    "    @staticmethod\n",
    "    def rescorla_wagner(q_val, epsilon_rew, epsilon_pun, epsilon_omi, reward):\n",
    "        if reward > 0: return q_val + epsilon_rew * (reward - q_val)\n",
    "        if reward < 0: return q_val + epsilon_pun * (reward - q_val)\n",
    "        return q_val + epsilon_omi * (reward - q_val)\n",
    "\n",
    "    @staticmethod\n",
    "    def reward(r_t, rho_rew, rho_pun):\n",
    "        if r_t > 0: return rho_rew\n",
    "        if r_t < 0: return rho_pun\n",
    "        return 0\n",
    "\n",
    "    def log_likelihood(self, cues, actions, rewards, epsilon_rew, epsilon_pun, epsilon_omi, rho_rew, rho_pun, bias_wth, bias_app):\n",
    "        n_stimuli = len(set(cues))\n",
    "        n_actions = len(set(actions))\n",
    "    \n",
    "        q_vals = np.zeros((n_stimuli, n_actions))\n",
    "    \n",
    "        log_likelihood = 0\n",
    "    \n",
    "        for t, a_t in enumerate(actions):\n",
    "            s_t = cues[t] - 1\n",
    "            r_t = self.reward(rewards[t], rho_rew, rho_pun)\n",
    "\n",
    "            qs = q_vals[s_t] + [ bias_wth, bias_app ]\n",
    "    \n",
    "            probs = self.softmax(qs)\n",
    "            log_likelihood += np.log(probs[a_t])\n",
    "    \n",
    "            # Update the Q-values using Rescorla-Wagner\n",
    "            q_vals[s_t, a_t] = self.rescorla_wagner(\n",
    "                q_val = q_vals[s_t, a_t],\n",
    "                epsilon_rew = epsilon_rew,\n",
    "                epsilon_pun = epsilon_pun,\n",
    "                epsilon_omi = epsilon_omi,\n",
    "                reward = r_t\n",
    "            )\n",
    "    \n",
    "        return log_likelihood\n",
    "    \n",
    "    @abstractmethod\n",
    "    def loss(self, params, cues, actions, rewards):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def minimize_loss(self, cues, actions, rewards):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, data):\n",
    "        fit_result = []\n",
    "        \n",
    "        for subject_id in data.ID.unique():\n",
    "            subject_data = data[ data.ID == subject_id ]\n",
    "            \n",
    "            cues = subject_data.cue.tolist()\n",
    "            actions = subject_data.pressed.tolist()\n",
    "            rewards = subject_data.outcome.tolist()\n",
    "            \n",
    "            loss, x = self.minimize_loss(cues, actions, rewards)\n",
    "            \n",
    "            x[\"ID\"] = subject_id\n",
    "            x[\"loss\"] = loss\n",
    "            fit_result.append(x)\n",
    "\n",
    "        fit_result = pd.concat(fit_result)\n",
    "        fit_result.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "        return fit_result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T23:51:27.725982Z",
     "start_time": "2024-12-06T23:51:27.724184Z"
    }
   },
   "id": "549ab08de8bd054c"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Model1(Model):\n",
    "    def loss(self, params, cues, actions, rewards):\n",
    "        epsilon, beta = params\n",
    "        return -self.log_likelihood(\n",
    "            cues = cues,\n",
    "            actions = actions,\n",
    "            rewards = rewards,\n",
    "            epsilon_rew = epsilon,\n",
    "            epsilon_pun = epsilon,\n",
    "            epsilon_omi = epsilon,\n",
    "            rho_rew = beta,\n",
    "            rho_pun = -beta,\n",
    "            bias_wth = 0,\n",
    "            bias_app = 0\n",
    "        )\n",
    "\n",
    "    def minimize_loss(self, cues, actions, rewards):\n",
    "        result = minimize(\n",
    "            fun = self.loss,\n",
    "            x0 = [0.5, 5],\n",
    "            bounds = [self.epsilon_bounds, self.beta_bounds],\n",
    "            args = (cues, actions, rewards),\n",
    "            method = \"Nelder-Mead\"\n",
    "        )\n",
    "        \n",
    "        fit_params = pd.DataFrame([result.x])\n",
    "        fit_params.columns = [\"epsilon\", \"beta\"]\n",
    "        \n",
    "        return result.fun, fit_params"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T23:51:27.729724Z",
     "start_time": "2024-12-06T23:51:27.727262Z"
    }
   },
   "id": "17d027020fbe962d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class Model2(Model):\n",
    "    def loss(self, params, cues, actions, rewards):\n",
    "        epsilon, rho_rew, rho_pun = params\n",
    "        return -self.log_likelihood(\n",
    "            cues = cues,\n",
    "            actions = actions,\n",
    "            rewards = rewards,\n",
    "            epsilon_rew = epsilon,\n",
    "            epsilon_pun = epsilon,\n",
    "            epsilon_omi = epsilon,\n",
    "            rho_rew = rho_rew,\n",
    "            rho_pun = -rho_pun,\n",
    "            bias_wth = 0,\n",
    "            bias_app = 0\n",
    "        )\n",
    "\n",
    "    def minimize_loss(self, cues, actions, rewards):\n",
    "        result = minimize(\n",
    "            fun = self.loss,\n",
    "            x0 = [0.5, 5, 5],\n",
    "            bounds = [\n",
    "                self.epsilon_bounds,\n",
    "                self.beta_bounds,\n",
    "                self.beta_bounds\n",
    "            ],\n",
    "            args = (cues, actions, rewards),\n",
    "            method = \"Nelder-Mead\"\n",
    "        )\n",
    "\n",
    "        fit_params = pd.DataFrame([result.x])\n",
    "        fit_params.columns = [\"epsilon\", \"rho_rew\", \"rho_pun\"]\n",
    "        fit_params[\"rho_pun\"] = -fit_params[\"rho_pun\"]\n",
    "\n",
    "        return result.fun, fit_params"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T23:51:27.733481Z",
     "start_time": "2024-12-06T23:51:27.730098Z"
    }
   },
   "id": "9f803ac2b52fcaec"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class Model3(Model):\n",
    "    def loss(self, params, cues, actions, rewards):\n",
    "        epsilon_rew, epsilon_pun, epsilon_omi, beta = params\n",
    "        return -self.log_likelihood(\n",
    "            cues = cues,\n",
    "            actions = actions,\n",
    "            rewards = rewards,\n",
    "            epsilon_rew = epsilon_rew,\n",
    "            epsilon_pun = epsilon_pun,\n",
    "            epsilon_omi = epsilon_omi,\n",
    "            rho_rew = beta,\n",
    "            rho_pun = -beta,\n",
    "            bias_wth = 0,\n",
    "            bias_app = 0\n",
    "        )\n",
    "\n",
    "    def minimize_loss(self, cues, actions, rewards):\n",
    "        result = minimize(\n",
    "            fun = self.loss,\n",
    "            x0 = [0.5, 0.5, 0.5, 5],\n",
    "            bounds = [\n",
    "                self.epsilon_bounds,\n",
    "                self.epsilon_bounds,\n",
    "                self.epsilon_bounds,\n",
    "                self.beta_bounds,\n",
    "            ],\n",
    "            args = (cues, actions, rewards),\n",
    "            method = \"Nelder-Mead\"\n",
    "        )\n",
    "\n",
    "        fit_params = pd.DataFrame([result.x])\n",
    "        fit_params.columns = [\"epsilon_rew\", \"epsilon_pun\", \"epsilon_omi\", \"beta\"]\n",
    "\n",
    "        return result.fun, fit_params"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T23:51:27.736914Z",
     "start_time": "2024-12-06T23:51:27.734002Z"
    }
   },
   "id": "269514632d884203"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class Model4(Model):\n",
    "    def loss(self, params, cues, actions, rewards):\n",
    "        epsilon, beta, bias_app, bias_wth = params\n",
    "        return -self.log_likelihood(\n",
    "            cues = cues,\n",
    "            actions = actions,\n",
    "            rewards = rewards,\n",
    "            epsilon_rew = epsilon,\n",
    "            epsilon_pun = epsilon,\n",
    "            epsilon_omi = epsilon,\n",
    "            rho_rew = beta,\n",
    "            rho_pun = -beta,\n",
    "            bias_wth = bias_wth,\n",
    "            bias_app = bias_app\n",
    "        )\n",
    "\n",
    "    def minimize_loss(self, cues, actions, rewards):\n",
    "        result = minimize(\n",
    "            fun = self.loss,\n",
    "            x0 = [0.5, 5, 0, 0],\n",
    "            bounds = [\n",
    "                self.epsilon_bounds,\n",
    "                self.beta_bounds,\n",
    "                self.bias_bounds,\n",
    "                self.bias_bounds,\n",
    "            ],\n",
    "            args = (cues, actions, rewards),\n",
    "            method = \"Nelder-Mead\"\n",
    "        )\n",
    "\n",
    "        fit_params = pd.DataFrame([result.x])\n",
    "        fit_params.columns = [\"epsilon\", \"beta\", \"bias_app\", \"bias_wth\"]\n",
    "\n",
    "        return result.fun, fit_params"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T23:51:27.740390Z",
     "start_time": "2024-12-06T23:51:27.736983Z"
    }
   },
   "id": "bc5b8ea197744ac2"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class Model5(Model):\n",
    "    def loss(self, params, cues, actions, rewards):\n",
    "        epsilon, rho_rew, rho_pun, bias_app, bias_wth = params\n",
    "        return -self.log_likelihood(\n",
    "            cues = cues,\n",
    "            actions = actions,\n",
    "            rewards = rewards,\n",
    "            epsilon_rew = epsilon,\n",
    "            epsilon_pun = epsilon,\n",
    "            epsilon_omi = epsilon,\n",
    "            rho_rew = rho_rew,\n",
    "            rho_pun = -rho_pun,\n",
    "            bias_wth = bias_wth,\n",
    "            bias_app = bias_app\n",
    "        )\n",
    "\n",
    "    def minimize_loss(self, cues, actions, rewards):\n",
    "        result = minimize(\n",
    "            fun = self.loss,\n",
    "            x0 = [0.5, 5, 5, 0, 0],\n",
    "            bounds = [\n",
    "                self.epsilon_bounds,\n",
    "                self.beta_bounds,\n",
    "                self.beta_bounds,\n",
    "                self.bias_bounds,\n",
    "                self.bias_bounds,\n",
    "            ],\n",
    "            args = (cues, actions, rewards),\n",
    "            method = \"Nelder-Mead\"\n",
    "        )\n",
    "\n",
    "        fit_params = pd.DataFrame([result.x])\n",
    "        fit_params.columns = [\"epsilon\", \"rho_rew\", \"rho_pun\", \"bias_app\", \"bias_wth\"]\n",
    "\n",
    "        return result.fun, fit_params"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T23:51:27.743542Z",
     "start_time": "2024-12-06T23:51:27.739974Z"
    }
   },
   "id": "d0a012cc69a9650"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class Model6(Model):\n",
    "    @staticmethod\n",
    "    def reward(a_t, r_t, rho_rew_app, rho_rew_wth, rho_pun_app, rho_pun_wth):\n",
    "        if a_t == 1 and r_t > 0: return rho_rew_app\n",
    "        if a_t == 1 and r_t < 0: return -rho_pun_app\n",
    "\n",
    "        if a_t == 0 and r_t > 0: return rho_rew_wth\n",
    "        if a_t == 0 and r_t < 0: return -rho_pun_wth\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def log_likelihood(self, cues, actions, rewards, epsilon_rew, epsilon_pun, epsilon_omi, rho_rew_app, rho_rew_wth, rho_pun_app, rho_pun_wth, bias_wth, bias_app):\n",
    "        n_stimuli = len(set(cues))\n",
    "        n_actions = len(set(actions))\n",
    "\n",
    "        q_vals = np.zeros((n_stimuli, n_actions))\n",
    "\n",
    "        log_likelihood = 0\n",
    "\n",
    "        for t, a_t in enumerate(actions):\n",
    "            s_t = cues[t] - 1\n",
    "            r_t = self.reward(a_t, rewards[t], rho_rew_app, rho_rew_wth, rho_pun_app, rho_pun_wth)\n",
    "\n",
    "            qs = q_vals[s_t] + [bias_wth, bias_app]\n",
    "\n",
    "            probs = self.softmax(qs)\n",
    "            log_likelihood += np.log(probs[a_t])\n",
    "\n",
    "            # Update the Q-values using Rescorla-Wagner\n",
    "            q_vals[s_t, a_t] = self.rescorla_wagner(\n",
    "                q_val = q_vals[s_t, a_t],\n",
    "                epsilon_rew = epsilon_rew,\n",
    "                epsilon_pun = epsilon_pun,\n",
    "                epsilon_omi = epsilon_omi,\n",
    "                reward = r_t\n",
    "            )\n",
    "\n",
    "        return log_likelihood\n",
    "\n",
    "    def loss(self, params, cues, actions, rewards):\n",
    "        epsilon, rho_rew_app, rho_rew_wth, rho_pun_app, rho_pun_wth, bias_app, bias_wth = params\n",
    "        return -self.log_likelihood(\n",
    "            cues = cues,\n",
    "            actions = actions,\n",
    "            rewards = rewards,\n",
    "            epsilon_rew = epsilon,\n",
    "            epsilon_pun = epsilon,\n",
    "            epsilon_omi = epsilon,\n",
    "            rho_rew_app = rho_rew_app,\n",
    "            rho_rew_wth = rho_rew_wth,\n",
    "            rho_pun_app = rho_pun_app,\n",
    "            rho_pun_wth = rho_pun_wth,\n",
    "            bias_wth = bias_wth,\n",
    "            bias_app = bias_app\n",
    "        )\n",
    "\n",
    "    def minimize_loss(self, cues, actions, rewards):\n",
    "        result = minimize(\n",
    "            fun = self.loss,\n",
    "            x0 = [0.5, 5, 5, 5, 5, 0, 0],\n",
    "            bounds = [\n",
    "                self.epsilon_bounds,\n",
    "                self.beta_bounds,\n",
    "                self.beta_bounds,\n",
    "                self.beta_bounds,\n",
    "                self.beta_bounds,\n",
    "                self.bias_bounds,\n",
    "                self.bias_bounds\n",
    "            ],\n",
    "            args = (cues, actions, rewards),\n",
    "            method = \"Nelder-Mead\"\n",
    "        )\n",
    "\n",
    "        fit_params = pd.DataFrame([result.x])\n",
    "        fit_params.columns = [\"epsilon\", \"rho_rew_app\", \"rho_rew_wth\", \"rho_pun_app\", \"rho_pun_wth\", \"bias_app\", \"bias_wth\"]\n",
    "        fit_params[\"rho_pun_app\"] = -fit_params[\"rho_pun_app\"]\n",
    "        fit_params[\"rho_pun_wth\"] = -fit_params[\"rho_pun_wth\"]\n",
    "\n",
    "        return result.fun, fit_params"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T23:51:27.748298Z",
     "start_time": "2024-12-06T23:51:27.745500Z"
    }
   },
   "id": "7fe4fbf28fc6d5f7"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class Model7(Model):\n",
    "    @staticmethod\n",
    "    def rescorla_wagner(q_val, epsilon_app, epsilon_wth, action, reward):\n",
    "        if action == 1:\n",
    "            return q_val + epsilon_app * (reward - q_val)\n",
    "\n",
    "        return q_val + epsilon_wth * (reward - q_val)\n",
    "    \n",
    "    def log_likelihood(self, cues, actions, rewards, epsilon_app, epsilon_wth, rho_rew, rho_pun, bias_wth, bias_app):\n",
    "        n_stimuli = len(set(cues))\n",
    "        n_actions = len(set(actions))\n",
    "\n",
    "        q_vals = np.zeros((n_stimuli, n_actions))\n",
    "\n",
    "        log_likelihood = 0\n",
    "\n",
    "        for t, a_t in enumerate(actions):\n",
    "            s_t = cues[t] - 1\n",
    "            r_t = self.reward(rewards[t], rho_rew, rho_pun)\n",
    "\n",
    "            qs = q_vals[s_t] + [ bias_wth, bias_app ]\n",
    "\n",
    "            probs = self.softmax(qs)\n",
    "            log_likelihood += np.log(probs[a_t])\n",
    "\n",
    "            # Update the Q-values using Rescorla-Wagner\n",
    "            q_vals[s_t, a_t] = self.rescorla_wagner(\n",
    "                q_val = q_vals[s_t, a_t],\n",
    "                epsilon_app = epsilon_app,\n",
    "                epsilon_wth = epsilon_wth,\n",
    "                action = a_t,\n",
    "                reward = r_t\n",
    "            )\n",
    "\n",
    "        return log_likelihood\n",
    "\n",
    "    def loss(self, params, cues, actions, rewards):\n",
    "        epsilon_app, epsilon_wth, rho_rew, rho_pun, bias_app, bias_wth = params\n",
    "        return -self.log_likelihood(\n",
    "            cues = cues,\n",
    "            actions = actions,\n",
    "            rewards = rewards,\n",
    "            epsilon_app = epsilon_app,\n",
    "            epsilon_wth = epsilon_wth,\n",
    "            rho_rew = rho_rew,\n",
    "            rho_pun = -rho_pun,\n",
    "            bias_wth = bias_wth,\n",
    "            bias_app = bias_app\n",
    "        )\n",
    "\n",
    "    def minimize_loss(self, cues, actions, rewards):\n",
    "        result = minimize(\n",
    "            fun = self.loss,\n",
    "            x0 = [0.5, 0.5, 5, 5, 0, 0],\n",
    "            bounds = [\n",
    "                self.epsilon_bounds,\n",
    "                self.epsilon_bounds,\n",
    "                self.beta_bounds,\n",
    "                self.beta_bounds,\n",
    "                self.bias_bounds,\n",
    "                self.bias_bounds\n",
    "            ],\n",
    "            args = (cues, actions, rewards),\n",
    "            method = \"Nelder-Mead\"\n",
    "        )\n",
    "\n",
    "        fit_params = pd.DataFrame([result.x])\n",
    "        fit_params.columns = [\"epsilon_app\", \"epsilon_wth\", \"rho_rew\", \"rho_pun\", \"bias_app\", \"bias_wth\"]\n",
    "        fit_params[\"rho_pun\"] = -fit_params[\"rho_pun\"]\n",
    "\n",
    "        return result.fun, fit_params"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T23:51:27.752089Z",
     "start_time": "2024-12-06T23:51:27.746959Z"
    }
   },
   "id": "143f5f6449eb2ff8"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class Model8(Model):\n",
    "    @staticmethod\n",
    "    def rescorla_wagner(q_val, epsilon_app, epsilon_wth, action, reward):\n",
    "        if action == 1:\n",
    "            return q_val + epsilon_app * (reward - q_val)\n",
    "\n",
    "        return q_val + epsilon_wth * (reward - q_val)\n",
    "\n",
    "    def log_likelihood(self, cues, actions, rewards, epsilon_app, epsilon_wth, rho_rew, rho_pun, bias_wth, bias_app, p):\n",
    "        n_stimuli = len(set(cues))\n",
    "        n_actions = len(set(actions))\n",
    "\n",
    "        q_vals = np.zeros((n_stimuli, n_actions))\n",
    "\n",
    "        log_likelihood = 0\n",
    "\n",
    "        for t, a_t in enumerate(actions):\n",
    "            s_t = cues[t] - 1\n",
    "            r_t = self.reward(rewards[t], rho_rew, rho_pun)\n",
    "\n",
    "            qs = q_vals[s_t] + [ bias_wth, bias_app ]\n",
    "            \n",
    "            max_q = np.max(q_vals[s_t])\n",
    "            if max_q < 0: qs[0] += p\n",
    "            if max_q > 0: qs[1] += p\n",
    "\n",
    "            probs = self.softmax(qs)\n",
    "            log_likelihood += np.log(probs[a_t])\n",
    "\n",
    "            # Update the Q-values using Rescorla-Wagner\n",
    "            q_vals[s_t, a_t] = self.rescorla_wagner(\n",
    "                q_val = q_vals[s_t, a_t],\n",
    "                epsilon_app = epsilon_app,\n",
    "                epsilon_wth = epsilon_wth,\n",
    "                action = a_t,\n",
    "                reward = r_t\n",
    "            )\n",
    "\n",
    "        return log_likelihood\n",
    "\n",
    "    def loss(self, params, cues, actions, rewards):\n",
    "        epsilon_app, epsilon_wth, rho_rew, rho_pun, bias_app, bias_wth, p = params\n",
    "        return -self.log_likelihood(\n",
    "            cues = cues,\n",
    "            actions = actions,\n",
    "            rewards = rewards,\n",
    "            epsilon_app = epsilon_app,\n",
    "            epsilon_wth = epsilon_wth,\n",
    "            rho_rew = rho_rew,\n",
    "            rho_pun = -rho_pun,\n",
    "            bias_wth = bias_wth,\n",
    "            bias_app = bias_app,\n",
    "            p = p\n",
    "        )\n",
    "\n",
    "    def minimize_loss(self, cues, actions, rewards):\n",
    "        result = minimize(\n",
    "            fun = self.loss,\n",
    "            x0 = [0.5, 0.5, 5, 5, 0, 0, 0.5],\n",
    "            bounds = [\n",
    "                self.epsilon_bounds,\n",
    "                self.epsilon_bounds,\n",
    "                self.beta_bounds,\n",
    "                self.beta_bounds,\n",
    "                self.bias_bounds,\n",
    "                self.bias_bounds,\n",
    "                self.p_bounds\n",
    "            ],\n",
    "            args = (cues, actions, rewards),\n",
    "            method = \"Nelder-Mead\"\n",
    "        )\n",
    "\n",
    "        fit_params = pd.DataFrame([result.x])\n",
    "        fit_params.columns = [\"epsilon_app\", \"epsilon_wth\", \"rho_rew\", \"rho_pun\", \"bias_app\", \"bias_wth\", \"p\"]\n",
    "        fit_params[\"rho_pun\"] = -fit_params[\"rho_pun\"]\n",
    "\n",
    "        return result.fun, fit_params"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T23:51:27.756133Z",
     "start_time": "2024-12-06T23:51:27.754299Z"
    }
   },
   "id": "cdeb12d782948cec"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"gen_data.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T23:51:27.761603Z",
     "start_time": "2024-12-06T23:51:27.756744Z"
    }
   },
   "id": "e914760d583ee66"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "2866.060590541449"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = Model1()\n",
    "res = model1.fit(data)\n",
    "res[\"loss\"].sum() # 2866"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T23:51:30.178599Z",
     "start_time": "2024-12-06T23:51:27.762134Z"
    }
   },
   "id": "98d819e739e21ca2"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "2862.9286280091674"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Model2()\n",
    "res = model2.fit(data)\n",
    "res[\"loss\"].sum() # 2862"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T23:51:34.753665Z",
     "start_time": "2024-12-06T23:51:30.176017Z"
    }
   },
   "id": "6304c5c3aec6d1b4"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "2792.919405767867"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = Model3()\n",
    "res = model3.fit(data)\n",
    "res[\"loss\"].sum() # 2793"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T23:51:46.189696Z",
     "start_time": "2024-12-06T23:51:34.755040Z"
    }
   },
   "id": "9c34d43b26d41fd"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "2736.4129286559555"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4 = Model4()\n",
    "res = model4.fit(data)\n",
    "res[\"loss\"].sum() # 2736"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T23:51:55.223924Z",
     "start_time": "2024-12-06T23:51:46.186985Z"
    }
   },
   "id": "dd23b143f678c346"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "2732.855646686895"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5 = Model5()\n",
    "res = model5.fit(data)\n",
    "res[\"loss\"].sum() # 2732"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T23:52:08.575203Z",
     "start_time": "2024-12-06T23:51:55.222249Z"
    }
   },
   "id": "424b19187c4ed43b"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "2655.651541220959"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model6 = Model6()\n",
    "res = model6.fit(data)\n",
    "res[\"loss\"].sum() # 2663 (2655)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T23:52:39.590950Z",
     "start_time": "2024-12-06T23:52:08.573014Z"
    }
   },
   "id": "c3e973f03492edad"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "2682.4008837954207"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model7 = Model7()\n",
    "res = model7.fit(data)\n",
    "res[\"loss\"].sum() # 2682"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T23:53:00.174658Z",
     "start_time": "2024-12-06T23:52:39.588047Z"
    }
   },
   "id": "3961fb73ca7a8cea"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "2653.68236585408"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model8 = Model8()\n",
    "res = model8.fit(data)\n",
    "res[\"loss\"].sum() # 2652"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T23:53:55.134351Z",
     "start_time": "2024-12-06T23:53:00.171254Z"
    }
   },
   "id": "e255b9dfbf482622"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T23:53:55.134454Z",
     "start_time": "2024-12-06T23:53:55.131599Z"
    }
   },
   "id": "5fcaa11aeac576d0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
