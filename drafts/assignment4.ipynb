{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-06T18:21:08.196020Z",
     "start_time": "2024-12-06T18:21:06.713297Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model fitting and Pavlovian biases. Pavlovian-instrumental interactions\n",
    "\n",
    "## Part 1\n",
    "\n",
    "Download the dataset .csv from Slack. It contains the data of 10 subjects (see column \"ID\" for the subject identifier), performing a go/no-go task, each for 600 trials. The column \"cue\" informs you about the presented trial type (see the \"cue mapping\" variable in our template). The column \"pressed\" contains the response of the participant (0 is no-go, 1 is go) and \"outcome\" contains whether a reward was delivered (1), nothing was delivered (0), a punishment was given (-1).\n",
    "\n",
    "Recreate figure 2E of the paper \"Go and no-go learning in reward and punishment: Interactions between affect and effect\" with the data you have. Only the bar plots are important here, no need for error bars or significance tests."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e16dd046f80f3418"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"gen_data.csv\")\n",
    "\n",
    "# Go+ = Go to win\n",
    "# Go- = go to avoid losing\n",
    "# NoGo+ = don't go to win\n",
    "# NoGo- = don't go to avoid losing\n",
    "cue_mapping = { 1: \"Go+\", 2: \"Go-\", 3: \"NoGo+\", 4: \"NoGo-\" }\n",
    "\n",
    "data[\"CueFactor\"] = [cue_mapping.get(cue) for cue in data.cue]\n",
    "data[\"CueFactor\"] = data[\"CueFactor\"].astype(\"category\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T18:21:08.204623Z",
     "start_time": "2024-12-06T18:21:08.197336Z"
    }
   },
   "id": "4b12f50839928ea0"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Determine if each trial is correct or not\n",
    "data[\"Correct\"] = False\n",
    "for i, trial in data.iterrows():\n",
    "    if trial.CueFactor == \"Go+\" or trial.CueFactor == \"Go-\":\n",
    "        data.loc[i, \"Correct\"] = trial.pressed == 1\n",
    "\n",
    "    if trial.CueFactor == \"NoGo+\" or trial.CueFactor == \"NoGo-\":\n",
    "        data.loc[i, \"Correct\"] = trial.pressed != 1\n",
    "\n",
    "# Compute the average of correct values for each cue\n",
    "cue_means = data.groupby([\"ID\", \"CueFactor\"], observed = False)[\"Correct\"].mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T18:21:08.584647Z",
     "start_time": "2024-12-06T18:21:08.205954Z"
    }
   },
   "id": "7af018355a2ab31d"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "means = []\n",
    "cues = []\n",
    "for cue in data.CueFactor.unique():\n",
    "    cue_data = cue_means.xs(cue, level = \"CueFactor\")\n",
    "    means.append(np.mean(cue_data))\n",
    "    cues.append(cue)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T18:21:08.590111Z",
     "start_time": "2024-12-06T18:21:08.586488Z"
    }
   },
   "id": "508eb7b91cc8b945"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(cues, means)\n",
    "plt.ylabel(\"Probability\\ncorrect\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(\n",
    "    ticks = cues,\n",
    "    labels = [\n",
    "        \"Go to\\nWin\",\n",
    "        \"Go to\\nAvoid\",\n",
    "        \"Nogo to\\nWin\",\n",
    "        \"Nogo to\\nAvoid\"\n",
    "    ]\n",
    ")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T18:21:08.680986Z",
     "start_time": "2024-12-06T18:21:08.588608Z"
    }
   },
   "id": "20fb9afb11cfdc92"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2\n",
    "\n",
    "Program the log likelihood functions of the models 1 to 7 (including) presented in \"Disentangling the Roles of Approach, Activation and Valence in Instrumental and Pavlovian Responding\" (see Table 2 of that paper for the model numbering and relevant parameters). The paper uses these parameters:\n",
    "\n",
    "- learning rate $\\epsilon$\n",
    "- feedback sensitivity $\\beta$\n",
    "- the general feedback sensitivity $\\beta$ can be replaced by separate reward and punishment sensitivities $\\rho$ (we don’t include a sensitivity for omission)\n",
    "- there can be different learning rates $\\epsilon$ for reward, feedback omission, and punishment (the paper doesn't make use of omissions, so they use only two learning rates, you will need three)\n",
    "- there can be a general bias to approach $bias_{app}$, and a general bias to withhold responding $bias_{wth}$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e8badb35a05eb50"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### General model\n",
    "\n",
    "$s^{\\mathcal{I}}_t$ - the instrumental stimulus, presented at trial $t$ (one out of four: Go+, Go-, NoGo+, and NoGo-).\n",
    "\n",
    "$a_t$ - the action (choice) at trial $t$. The action can be one of four types: in the withdrawal block, (1) go withdrawal and (2) nogo withdrawal, and in the approach block, (3) go approach and (4) nogo approach.\n",
    "\n",
    "$r_t$ - the reinforcement obtained, $r_t \\in \\{-1, 0, 1\\}$ where $-1$ marks a punishment, $0$ marks no reinforcement (feedback omission), and $-1$ marks a reward.\n",
    "\n",
    "The probability of action $a_t$ in the presence of stimulus $s^{\\mathcal{I}}_t$ is a standard probabilistic function:\n",
    "\n",
    "$$\n",
    "p(a_t | $s^{\\mathcal{I}}_t$) = \\frac{exp(\\mathcal{W}^{\\mathcal{I}} ($s^{\\mathcal{I}}_t$, a_t))}{\\sum_{a'} exp(\\mathcal{W}^{\\mathcal{I}} ($s^{\\mathcal{I}}_t$, a'))}\n",
    "$$\n",
    "\n",
    "Here, $\\mathcal{W}^{\\mathcal{I}}$ is the instrumental weight of action $a_t$:\n",
    "\n",
    "$$\n",
    "\\mathcal{W}^{\\mathcal{I}}(s^{\\mathcal{I}}_t, a_t) = \\mathcal{Q}(s^{\\mathcal{I}}_t, a_t) + b(a_t)\n",
    "$$\n",
    "\n",
    "The variable $b(a_t)$ can take on value $bias_{wth}$ for withhold actions, or $bias_{app}$ for approach actions.\n",
    "\n",
    "The Q-values are updated according to a Rescorla-Wagner-like rule with a fixed learning rate $\\epsilon$. The immediate, intrinsic, value of the reinforcements may have a different meaning for different subjects. To measure this effect, two further parameters are added: $\\rho_{rew}$ for the reward sensitivity and $\\rho_{pun}$ for the punishment sensitivity. Update equation for the expectations is thus:\n",
    "\n",
    "$$\n",
    "\\mathcal{Q}_{t+1}(s^{\\mathcal{I}}_t, a_t) = \\mathcal{Q}_{t}(s^{\\mathcal{I}}_t, a_t) + \\epsilon (\\mathcal{R}_t - \\mathcal{Q}_{t}(s^{\\mathcal{I}}_t, a_t))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{R}_t =\\begin{cases}\n",
    "      \\rho_{rew} & \\text{ if }\\ r_t > 0 \\\\\n",
    "      \\rho_{pun} & \\text{ if }\\ r_t < 0 \\\\\n",
    "    \\end{cases}\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88f2f0b6cf51dafe"
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp = np.exp(x)\n",
    "    return exp / np.sum(exp)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T18:43:15.909415Z",
     "start_time": "2024-12-06T18:43:15.891767Z"
    }
   },
   "id": "d2c01097650a2a44"
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "def rescorla_wagner(q_val, epsilon_rew, epsilon_pun, epsilon_omi, reward):\n",
    "    \"\"\"\n",
    "    Recalculates Q-value based on received reward and provided learning rates.\n",
    "    :param q_val: current Q-value\n",
    "    :param epsilon_rew: learning rate (reward)\n",
    "    :param epsilon_pun: learning rate (punishment)\n",
    "    :param epsilon_omi: learning rate (omission)\n",
    "    :param reward: reward\n",
    "    :return: updated Q-value\n",
    "    \"\"\"\n",
    "    if reward > 0:\n",
    "        return q_val + epsilon_rew * (reward - q_val)\n",
    "\n",
    "    if reward < 0:\n",
    "        return q_val + epsilon_pun * (reward - q_val)\n",
    "\n",
    "    return q_val + epsilon_omi * (reward - q_val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T18:39:59.285561Z",
     "start_time": "2024-12-06T18:39:59.137543Z"
    }
   },
   "id": "23bd2b8e3214b096"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "def reward(a_t, r_t, params):\n",
    "    rho_rew, rho_pun = params\n",
    "    if r_t > 0: return rho_rew\n",
    "    if r_t < 0: return rho_pun\n",
    "    return 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T18:32:48.573976Z",
     "start_time": "2024-12-06T18:32:48.571449Z"
    }
   },
   "id": "d9ed8a6116b7f59f"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "def bias(a, bias_app, bias_wth):\n",
    "    \"\"\"\n",
    "    Calculates response bias\n",
    "    \n",
    "    :param a: action taken (1 - approach, 0 - withhold)\n",
    "    :param bias_app: approach action bias\n",
    "    :param bias_wth: withhold action bias\n",
    "    :return: response bias\n",
    "    \"\"\"\n",
    "    return bias_app if a == 1 else bias_wth"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T18:32:48.996538Z",
     "start_time": "2024-12-06T18:32:48.990157Z"
    }
   },
   "id": "d5f9f2c7fd481344"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def weight(q_val, a, bias_app, bias_wth):\n",
    "    return q_val + bias(a, bias_app = bias_app, bias_wth = bias_wth)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T18:32:49.307131Z",
     "start_time": "2024-12-06T18:32:49.301552Z"
    }
   },
   "id": "58dc524d0dfa6365"
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [],
   "source": [
    "def log_likelihood(cues, actions, rewards, epsilon_rew, epsilon_pun, epsilon_omi, bias_app, bias_wth, reward_fun, reward_fun_params):\n",
    "    \"\"\" Calculate log-likelihood\n",
    "    \n",
    "    :param cues: presented stimuli\n",
    "    :param actions: performed actions\n",
    "    :param rewards: gained rewards\n",
    "    :param epsilon_rew: learning rate (reward)\n",
    "    :param epsilon_pun: learning rate (punishment)\n",
    "    :param epsilon_omi: learning rate (omission)\n",
    "    :param bias_app: approach bias\n",
    "    :param bias_wth: withhold bias\n",
    "    :param reward_fun: function to calculate the reward\n",
    "    :param reward_fun_params: parameters to pass to the reward function\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    n_stimuli = len(set(cues))\n",
    "    n_actions = len(set(actions))\n",
    "\n",
    "    q_vals = np.zeros((n_stimuli, n_actions))\n",
    "    w_vals = np.zeros((n_stimuli, n_actions))\n",
    "\n",
    "    log_likelihood = 0\n",
    "\n",
    "    for t, a_t in enumerate(actions):\n",
    "        s_t = cues[t] - 1\n",
    "        r_t = reward_fun(a_t, rewards[t], reward_fun_params)\n",
    "\n",
    "        probs = softmax(w_vals[s_t] * [0, 1])\n",
    "\n",
    "        log_likelihood += np.sum(np.log(probs))\n",
    "\n",
    "        # Update the Q-values using Rescorla-Wagner\n",
    "        q_vals[s_t, a_t] = rescorla_wagner(\n",
    "            q_val = q_vals[s_t, a_t],\n",
    "            epsilon_rew = epsilon_rew,\n",
    "            epsilon_pun = epsilon_pun,\n",
    "            epsilon_omi = epsilon_omi,\n",
    "            reward = r_t\n",
    "        )\n",
    "\n",
    "        # Update weights based on the Q-values and response bias\n",
    "        w_vals[s_t, a_t] = weight(q_vals[s_t, a_t], a = a_t, bias_app = bias_app, bias_wth = bias_wth)\n",
    "        w_vals[s_t, 1 - a_t] = weight(q_vals[s_t, 1 - a_t], a = 1 - a_t, bias_app = bias_app, bias_wth = bias_wth)\n",
    "\n",
    "    return log_likelihood"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T19:09:47.385131Z",
     "start_time": "2024-12-06T19:09:47.351036Z"
    }
   },
   "id": "b4a2d817e7180d6d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model 1\n",
    "\n",
    "Model 1 assumes that $-\\rho_{pun} = \\rho_{rew} = \\beta$ and that $bias_{wth} = bias_{app} = 0$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "447065d85c66decd"
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [],
   "source": [
    "def model1(cues, actions, rewards, epsilon, beta):\n",
    "    \"\"\"\n",
    "    Log-likelihood for model 1\n",
    "    :param cues: \n",
    "    :param actions: \n",
    "    :param rewards: \n",
    "    :param epsilon: learning rate\n",
    "    :param beta: feedback sensitivity\n",
    "    :return: \n",
    "    \"\"\"\n",
    "\n",
    "    return log_likelihood(\n",
    "        cues = cues,\n",
    "        actions = actions,\n",
    "        rewards = rewards,\n",
    "        epsilon_rew = epsilon,\n",
    "        epsilon_pun = epsilon,\n",
    "        epsilon_omi = epsilon,\n",
    "        bias_app = 0,\n",
    "        bias_wth = 0,\n",
    "        reward_fun = reward,\n",
    "        reward_fun_params = (beta, -beta)\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T19:09:48.255397Z",
     "start_time": "2024-12-06T19:09:48.231779Z"
    }
   },
   "id": "68359bb7cadf4e30"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model 2\n",
    "\n",
    "Model 2 includes separate reward and punishment sensitivities $\\rho_{rew}$ and $\\rho_{pun}$ with no action bias $bias_{wth} = bias_{app} = 0$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3baf3a14f1dc152"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "def model2(cues, actions, rewards, epsilon, rho_rew, rho_pun):\n",
    "    \"\"\"\n",
    "    Log-likelihood for model 2\n",
    "    :param cues: \n",
    "    :param actions: \n",
    "    :param rewards: \n",
    "    :param epsilon: learning rate\n",
    "    :param rho_rew: feedback sensitivity (reward)\n",
    "    :param rho_pun: feedback sensitivity (punishment)\n",
    "    :return: \n",
    "    \"\"\"\n",
    "\n",
    "    return log_likelihood(\n",
    "        cues = cues,\n",
    "        actions = actions,\n",
    "        rewards = rewards,\n",
    "        epsilon_rew = epsilon,\n",
    "        epsilon_pun = epsilon,\n",
    "        epsilon_omi = epsilon,\n",
    "        bias_app = 0,\n",
    "        bias_wth = 0,\n",
    "        reward_fun = reward,\n",
    "        reward_fun_params = (rho_rew, rho_pun)\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T18:35:29.469186Z",
     "start_time": "2024-12-06T18:35:29.461487Z"
    }
   },
   "id": "ee5c201b3fd0e715"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model 3\n",
    "\n",
    "Model 3 again assumes $-\\rho_{pun} = \\rho_{rew} = \\beta$, and that $bias_{wth} = bias_{app} = 0$, but allows for two separate learning rates, i.e. $\\epsilon$ is replaced by $\\epsilon_{rew}$ on trials where $r_t = 1$, by $\\epsilon_{pun}$ on trials where $r_t = -1$, and by $\\epsilon_{omi}$ on trials where $r_t = 0$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7d9a00e4e3c847e"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "def model3(cues, actions, rewards, epsilon_rew, epsilon_pun, epsilon_omi, beta):\n",
    "    \"\"\"\n",
    "    Log-likelihood for model 3\n",
    "    :param cues: \n",
    "    :param actions: \n",
    "    :param rewards: \n",
    "    :param epsilon_rew: learning rate (reward)\n",
    "    :param epsilon_pun: learning rate (punishment)\n",
    "    :param epsilon_omi: learning rate (omission)\n",
    "    :param beta: feedback sensitivity\n",
    "    :return: \n",
    "    \"\"\"\n",
    "\n",
    "    return log_likelihood(\n",
    "        cues = cues,\n",
    "        actions = actions,\n",
    "        rewards = rewards,\n",
    "        epsilon_rew = epsilon_rew,\n",
    "        epsilon_pun = epsilon_pun,\n",
    "        epsilon_omi = epsilon_omi,\n",
    "        bias_app = 0,\n",
    "        bias_wth = 0,\n",
    "        reward_fun = reward,\n",
    "        reward_fun_params = (beta, -beta)\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T18:35:29.832840Z",
     "start_time": "2024-12-06T18:35:29.828007Z"
    }
   },
   "id": "703c432c35e064e4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model 4\n",
    "\n",
    "Model 4 assumes a common learning rate $\\epsilon$ and $-\\rho_{pun} = \\rho_{rew} = \\beta$, but includes action biases for withdrawal actions $bias_{wth}$ and approach actions $bias_{app}$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfb1babfa5d6437f"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "def model4(cues, actions, rewards, epsilon, beta, bias_app, bias_wth):\n",
    "    \"\"\"\n",
    "    Log-likelihood for model 4\n",
    "    :param cues: presented stimuli\n",
    "    :param actions: performed actions\n",
    "    :param rewards: gained rewards\n",
    "    :param epsilon: learning rate\n",
    "    :param beta: feedback sensitivity\n",
    "    :param bias_app: approach bias\n",
    "    :param bias_wth: withhold bias\n",
    "    :return: \n",
    "    \"\"\"\n",
    "\n",
    "    return log_likelihood(\n",
    "        cues = cues,\n",
    "        actions = actions,\n",
    "        rewards = rewards,\n",
    "        epsilon_rew = epsilon,\n",
    "        epsilon_pun = epsilon,\n",
    "        epsilon_omi = epsilon,\n",
    "        bias_app = bias_app,\n",
    "        bias_wth = bias_wth,\n",
    "        reward_fun = reward,\n",
    "        reward_fun_params = (beta, -beta)\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T18:35:30.141099Z",
     "start_time": "2024-12-06T18:35:30.135534Z"
    }
   },
   "id": "18ebde18ddb79c30"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model 5\n",
    "\n",
    "Model 5 assumes a common learning rate $\\epsilon$ with reward and punishment sensitivities $\\rho_{rew}$ and $\\rho_{pun}$, and includes action biases for withdrawal actions $bias_{wth}$ and approach actions $bias_{app}$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba73c82b1d9ddaba"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "def model5(cues, actions, rewards, epsilon, rho_rew, rho_pun, bias_app, bias_wth):\n",
    "    \"\"\"\n",
    "    Log-likelihood for model 5\n",
    "    :param cues: presented stimuli\n",
    "    :param actions: performed actions\n",
    "    :param rewards: gained rewards\n",
    "    :param epsilon: learning rate\n",
    "    :param rho_rew: feedback sensitivity (reward)\n",
    "    :param rho_pun: feedback sensitivity (punishment)\n",
    "    :param bias_app: approach bias\n",
    "    :param bias_wth: withhold bias\n",
    "    :return: \n",
    "    \"\"\"\n",
    "\n",
    "    return log_likelihood(\n",
    "        cues = cues,\n",
    "        actions = actions,\n",
    "        rewards = rewards,\n",
    "        epsilon_rew = epsilon,\n",
    "        epsilon_pun = epsilon,\n",
    "        epsilon_omi = epsilon,\n",
    "        bias_app = bias_app,\n",
    "        bias_wth = bias_wth,\n",
    "        reward_fun = reward,\n",
    "        reward_fun_params = (rho_rew, rho_pun)\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T18:35:30.453707Z",
     "start_time": "2024-12-06T18:35:30.448103Z"
    }
   },
   "id": "f248296c915699c7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model 6\n",
    "\n",
    "Model 6 allows for separate reward and punishment sensitivities for the approach and withhold actions: $\\rho_{rew}^{app}$, $\\rho_{rew}^{wth}$,  $\\rho_{pun}^{app}$, and $\\rho_{pun}^{wth}$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca7d847866a2dd1c"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "def reward_2(a_t, r_t, params):\n",
    "    rho_rew_app, rho_rew_wth, rho_pun_app, rho_pun_wth = params\n",
    "    if a_t == 1 and r_t > 0: return rho_rew_app\n",
    "    if a_t == 0 and r_t > 0: return rho_rew_wth\n",
    "\n",
    "    if a_t == 1 and r_t < 0: return rho_pun_app\n",
    "    if a_t == 0 and r_t < 0: return rho_pun_wth\n",
    "\n",
    "    return 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T18:35:31.055864Z",
     "start_time": "2024-12-06T18:35:31.051256Z"
    }
   },
   "id": "5c863805c49110e8"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "def model6(cues, actions, rewards, epsilon, rho_rew_app, rho_rew_wth, rho_pun_app, rho_pun_wth, bias_app, bias_wth):\n",
    "    \"\"\"\n",
    "    Log-likelihood for model 6\n",
    "    :param cues: presented stimuli\n",
    "    :param actions: performed actions\n",
    "    :param rewards: gained rewards\n",
    "    :param epsilon: learning rate\n",
    "    :param rho_rew_app: feedback sensitivity (reward for approach)\n",
    "    :param rho_rew_wth: feedback sensitivity (reward for withhold)\n",
    "    :param rho_pun_app: feedback sensitivity (punishment for approach)\n",
    "    :param rho_pun_wth: feedback sensitivity (punishment for withhold)\n",
    "    :param bias_app: approach bias\n",
    "    :param bias_wth: withhold bias\n",
    "    :return: \n",
    "    \"\"\"\n",
    "\n",
    "    return log_likelihood(\n",
    "        cues = cues,\n",
    "        actions = actions,\n",
    "        rewards = rewards,\n",
    "        epsilon_rew = epsilon,\n",
    "        epsilon_pun = epsilon,\n",
    "        epsilon_omi = epsilon,\n",
    "        bias_app = bias_app,\n",
    "        bias_wth = bias_wth,\n",
    "        reward_fun = reward_2,\n",
    "        reward_fun_params = (rho_rew_app, rho_rew_wth, rho_pun_app, rho_pun_wth)\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T18:35:31.425238Z",
     "start_time": "2024-12-06T18:35:31.414181Z"
    }
   },
   "id": "11744c927aca73da"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 3\n",
    "\n",
    "Create an additional model which takes into account Pavlovian biases. Use model 7 as a starting point for this. Add a parameter p to the model. To determine the action values to put into the softmax function for a given cue, take the Q-values, add the general bias to approach or withhold (as in equation 1 of the paper), and add p to the Q-value for approaching if the maximum Q-value for the current cue is positive, or add p to the Q-value for withholding if the maximal Q-value for the current cue is negative."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "854dd88bb934e99"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T18:35:32.004068Z",
     "start_time": "2024-12-06T18:35:31.998431Z"
    }
   },
   "id": "a8fed0a2d045fe0a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 4\n",
    "\n",
    "Optimize the models, by fitting all the parameters of each model to each individual subject, using the scipy minimize function. Pay attention to initialize the parameters to reasonable values and set sensible bounds for each parameter (since Q-values get turned into probabilities through a softmax, which uses an exponential function, you may have to limit some of the parameters to certain magnitudes, to prevent overflow errors). Given the number of models this can take some minutes, to save time you can e.g. only apply the logarithm at the end, rather than during every iteration of your for-loop."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ddd4349e7c800e6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4dc5e750e615a46"
  },
  {
   "cell_type": "markdown",
   "source": [
    "w(s, a) = q(s, a) + b_wtf/app + p <- depending on the q value"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f7a0324d2680408"
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T18:44:59.671663Z",
     "start_time": "2024-12-06T18:44:59.668075Z"
    }
   },
   "id": "2107eadd187c531e"
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [],
   "source": [
    "epsilon_bounds = (0.00001, 0.99999)\n",
    "beta_bounds = (0.1, 9.9999)\n",
    "# biases can be negative!"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T19:09:57.732752Z",
     "start_time": "2024-12-06T19:09:57.707323Z"
    }
   },
   "id": "9b0bf26231030fd4"
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [],
   "source": [
    "def loss1(params, cues, actions, rewards):\n",
    "    epsilon, beta = params\n",
    "    return -model1(cues, actions, rewards, epsilon, beta)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T19:09:57.917319Z",
     "start_time": "2024-12-06T19:09:57.901391Z"
    }
   },
   "id": "1c83480da14d0e92"
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [
    {
     "data": {
      "text/plain": "-878.0106431244467"
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_loss = []\n",
    "\n",
    "# for id in data.ID.unique():\n",
    "subject_id = data.ID.unique()[0]\n",
    "subject = data[ data.ID == subject_id ]\n",
    "\n",
    "cues = subject.cue.tolist()\n",
    "actions = subject.pressed.tolist()\n",
    "rewards = subject.outcome.tolist()\n",
    "\n",
    "model1(\n",
    "    cues = cues,\n",
    "    actions = actions,\n",
    "    rewards = rewards,\n",
    "    epsilon = 0.1,\n",
    "    beta = 1\n",
    ")\n",
    "\n",
    "# res = minimize(\n",
    "#     fun = loss1,\n",
    "#     x0 = [0.1, 5],\n",
    "#     bounds = [epsilon_bounds, beta_bounds],\n",
    "#     args = (cues, actions, rewards),\n",
    "#     method = \"Nelder-Mead\"\n",
    "# )\n",
    "\n",
    "# min_loss.append(res.fun)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-06T19:09:58.307612Z",
     "start_time": "2024-12-06T19:09:58.299167Z"
    }
   },
   "id": "47748d209e1d58"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 5\n",
    "\n",
    "Sum up the optimized log-likelihoods across all subjects for each model. Use this and all other relevant values to compute the BIC score for each model (using e.g. the BIC equation of Wikipedia). What does this tell you about which model describes the data best?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96df9c34bbd96727"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def bic(k, n, l):\n",
    "    \"\"\"\n",
    "    Calculate BIC criteria.\n",
    "    :param k: number of parameters estimated by the model.\n",
    "    :param n: sample size.\n",
    "    :param l: maximized value of the likelihood function of the model\n",
    "    :return: BIC\n",
    "    \"\"\"\n",
    "    return k * np.log(n) - 2 * np.log(l)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-05T14:24:19.633122Z",
     "start_time": "2024-12-05T14:24:19.628332Z"
    }
   },
   "id": "957f78fa1b18959d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
